# Application Portability
On occasion it may required to move an application off of a cluster or ensure that no traffic is routed to the cluster. To do this we will modify the deployment values within the deployment overlays that were used when we created the pacman application in the last lab.

Before we begin we will validate that every cluster is running the pacman application.
~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           6m21s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     0            0           6m21s
~~~

We would first like to ensure that cluster1 does not have any replicas running at all. To do this we will modify the file `~/federation-dev/labs/lab-7-assets/overlays/cluster1/deployment.yaml`.

~~~sh
cd ~/federation-dev/labs/lab-7-assets/overlays/cluster1
sed -i 's/replicas: 1/replicas: 0/g' deployment.yaml
git commit -am 'removal of cluster1'
git push origin master
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are load balanced across all three clusters.

## Moving the Application using Placement policies

Another approach will be modifying the *pacman* application deployment placement. This is done by modifying the `federateddeployment`
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster2"}]}}}'
~~~

The above commands states that the Pacman deployment should be present only in cluster2. To verify no deployments
are available in the other clusters the following command can be ran.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
No resources found.
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           8m23s
*** cluster3 ***
No resources found.
~~~

To populate the application back to all clusters, modify the placement to include cluster1 and cluster2.
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster1"}, {"name":"cluster2"}, {"name":"cluster3"}]}}}'
~~~

Verify the deployments are created again in all three clusters.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           24s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           9m24s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           25s
~~~

Can you spot the difference between scaling a deployment to 0 in a cluster using `overrides` and removing a deployment from a cluster using `placement`?

The most important thing to note during the modification of which clusters are running the
*pacman* application is that the scores persist regardless of which cluster the application is running and HAProxy always ensures the application is available.

Next Lab: [Lab 9 - Disaster Recovery](./9.md)<br>
Previous Lab: [Lab 6 - Deploying Pacman](./6.md)<br>
[Home](./README.md)
