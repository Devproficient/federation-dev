# Disaster Recovery

As we have seen, the strength of GitOps is the ability to mange application workloads and move applications between clusters.

Through the use of overlays, GitoOps can be used to provide `Disaster Recovery` solutions for our applications.

In this lab we are going to see how GitOps along with our MongoDB cluster manages a failure of the primary MongoDB Replica.

## Creating some chaos

We are going to patch the cluster1 overlay `deployment.yaml` of MongoDB application so we will scale the deployment to 0 in `cluster1` which is the cluster running the _PRIMARY_ ReplicaSet member.

Modify the deployment.yaml for cluster1 and commit the changes.
~~~sh
cd ~/federation-dev/labs/
sed -i 's/replicas: 1/replicas: 0/g' lab-6-assets/overlays/cluster1/deployment.yaml
git commit -am 'removal of cluster1 mongo replica'
git push origin master
argocd app sync cluster1-mongo
~~~

The above command states that there should be 0 replicas in cluster1. To verify
no pods are running the following command can be ran.

> **NOTE:** It is possible that you see the pod in terminating status, that's fine.

~~~sh
oc --context=cluster1 -n mongo get deployment

NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   0/0     0            0           4m37s
~~~

Let's go ahead and remove the storage for MongoDB from `cluster1`:

~~~sh
oc --context=cluster1 -n mongo delete pvc mongo
~~~

## Verifying Pacman Application still works

We have lost our primary MongoDB replica, but that didn't impact our application at all. Since we have three MongoDB replicas, the Pacman application can continue saving and reading high scores from the database.

You can go ahead and play Pacman, verify that high scores are saved.

## Bring the MongoDB replica back

Our engineers have been working hard during the weekend, we are ready to bring MongoDB replica on cluster1 back to life.

We will patch the deployment again and sync the app which will recreate the storage and redeploy the Mongo Replica
~~~sh
cd ~/federation-dev/labs/
sed -i 's/replicas: 0/replicas: 1/g' lab-6-assets/overlays/cluster1/deployment.yaml
git commit -am 'removal of cluster1 mongo replica'
git push origin master
argocd app sync cluster1-mongo
~~~

We should see our MongoDB pod being created:

~~~sh 
oc --context=cluster1 -n mongo get pods

NAME                     READY   STATUS              RESTARTS   AGE
mongo-5f9ff55b67-5bgtc   0/1     ContainerCreating   0          8s
~~~

Once the pod is running the MongoDB replica will be reconfigured, we can get the new primary and secondary members by running:

~~~sh
wait-for-deployment cluster1 mongo mongo
MONGO_POD=$(oc --context=cluster1 -n mongo get pod --selector="name=mongo" --output=jsonpath='{.items..metadata.name}')
REPLICASET_STATUS=$(oc --context=cluster1 -n mongo exec $MONGO_POD -- bash -c 'mongo --norc --quiet --username=admin --password=$MONGODB_ADMIN_PASSWORD --host localhost admin --tls --tlsCAFile /opt/mongo-ssl/ca.pem --eval "JSON.stringify(rs.status())"')
# Get Primary Member
echo $REPLICASET_STATUS | jq '.members[] | select(.state | contains(1)).name'
# Get Secondary Members
echo $REPLICASET_STATUS | jq '.members[] | select(.state | contains(2)).name'
~~~

That concludes the disaster recovery lab.

Next Lab: [Lab 10 - Wrapup](./10.md)<br>
Previous Lab: [Lab 9 - Replica Scheduling](./9.md)<br>
[Home](./README.md)
