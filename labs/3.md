<a id="markdown-federation-introduction" name="federation-introduction"></a>
## Federation Introduction

KubeFed identifies two cluster types:

1. **Host Cluster**: Cluster where the KubeFed Controller Manager is running
2. **Member Cluster**: Cluster being managed by KubeFed Controller Manager

> **NOTE:** A Host Cluster can be a Member Cluster as well, in fact, we are going to use this approach in our lab

KubeFed can be configured to work in two different ways:

1. **Cluster Scoped**: KubeFed can potentially manage all objects across our clusters
2. **Namespace Scoped**: KubeFed can manage only the content of a given namespace across our clusters

When to use one or another it's a good question and the answer will depend on what your use case is. As a rule of thumb
we will use Cluster Scoped when we want to manage not-namespaced objects like `ClusterRoles`, `ClusterRoleBindings`, `SecurityContextConstraints`, etc. 

Keep in mind that _Namespace Scoped_ Federation requires a KubeFed Controller Manager on each Namespace you want to federated.

<a id="markdown-deploy-federation" name="deploy-cluster-scoped-federation"></a>
## Deploy Cluster Scoped Federation

We do not need to deploy KubeFed on the clusters we want to manage, we could deploy KubeFed in an cluster external to our Federation environment for example. In this lab, for convenience, we are using one of the clusters (`cluster1`) as the _Host Cluster_.

KubeFed is deployed using an [Operator](https://coreos.com/operators/), for your convenience, the KubeFed Operator has been
already deployed in `Cluster1`. For more information about the KubeFed Operator, you can visit the [OperatorHub Page](https://operatorhub.io/operator/kubefed-operator).

You should see the KubeFed Operator running in the namespace `kube-federation-system`

~~~sh
oc --context=cluster1 -n kube-federation-system get pods

NAME                                             READY     STATUS    RESTARTS   AGE
kubefed-operator-5c5d57484d-tvrcb                1/1       Running   0          3m18s
~~~

Now we need to create a KubeFed resource to instantiate the KubeFed Controller in `cluster` scoped mode:

~~~sh
cat <<-EOF | oc --context=cluster1 apply -n kube-federation-system -f -
---
apiVersion: operator.kubefed.io/v1alpha1
kind: KubeFed
metadata:
  name: kubefed-resource
spec:
  scope: Cluster
---
EOF
~~~

The KubeFed Controller should be deployed now:

~~~sh
wait-for-deployment cluster1 kube-federation-system kubefed-controller-manager
oc --context=cluster1 -n kube-federation-system get pods

NAME                                             READY     STATUS    RESTARTS   AGE
kubefed-operator-5c5d57484d-tvrcb                1/1       Running   0          3m18s
kubefed-controller-manager-7dfcffb798-flbcm      1/1       Running   0          30s
kubefed-controller-manager-7dfcffb798-vzcwg      1/1       Running   0          31s
~~~

<a id="markdown-enabling-the-required-types" name="enabling-the-required-types"></a>
## Enabling the required Types

In order to be able to _federate_ our applications, we need to enable the types used by our applications. That means if we 
need to _federate_ a Deployment across our clusters we need first to get a new type called `FederatedDeployment` which is
managed by KubeFed and allow us to propagate a Deployment across our clusters.

When enabling types the `kubefedctl` tool will create some [CRDs](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-resources) under the hood. The way it works is:

1. Using `kubefedctl` you _enable_ a standard type like `secrets`
2. `kubefedctl` will go ahead and create the required CRDs
3. `kubefedctl` will enable the propagation of the new type
4. The KubeFed Controller Manager will detect the new `FederatedTypeConfig` that was created and will launch a controller **[1]** to take care of it

**[1]** In Kubernetes, a controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state

Now we are going to enable some of the federated types needed for our demo application:

~~~sh
for type in namespaces ingresses.extensions secrets serviceaccounts services configmaps persistentvolumeclaims deployments.apps clusterrolebindings.rbac.authorization.k8s.io clusterroles.rbac.authorization.k8s.io securitycontextconstraints
do
    kubefedctl enable $type --host-cluster-context=cluster1
done
~~~

<a id="markdown-register-the-clusters" name="register-the-clusters"></a>
## Register the clusters

Verify that there are no clusters registered yet (but note
that you can already reference the CRDs for KubeFed clusters):

~~~sh
oc --context=cluster1 -n kube-federation-system get kubefedclusters

No resources found.
~~~

Now use the `kubefedctl` tool to register (*join*) the three clusters:

~~~sh
kubefedctl join cluster1 --cluster-context cluster1 --host-cluster-context cluster1 --v=2
kubefedctl join cluster2 --cluster-context cluster2 --host-cluster-context cluster1 --v=2
kubefedctl join cluster3 --cluster-context cluster3 --host-cluster-context cluster1 --v=2
~~~

Note that the names of the clusters (`cluster1`, `cluster2` and `cluster3`) in the commands above are a reference to the contexts configured in the `oc` client. For this process to work as expected you need to make sure that the [client contexts](./2.md#configure-client-context-for-cluster-admin-access) have been properly configured with the right access levels and context names. The `--cluster-context` option for `kubefedctl join` can be used to override the reference to the client context configuration. When the option is not present, `kubefedctl` uses the cluster name to identify the client context.

Verify that the federated clusters are registered and in a ready state (this
can take a moment):

~~~sh
oc --context=cluster1 -n kube-federation-system get kubefedclusters

NAME       READY     AGE
cluster1   True      28s
cluster2   True      21s
cluster3   True      23s
~~~

Next Lab: [Lab 4 - Example Application One](./4.md)<br>
Previous Lab: [Lab 2 - Configure OpenShift client context for cluster admin access](./2.md)<br>
[Home](./README.md)